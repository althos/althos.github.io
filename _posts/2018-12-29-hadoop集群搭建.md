---
layout:     post
title:      hadood入门之环境搭建
subtitle:   hadoop完全集群搭建
date:       2018-12-29
author:     Cyber
header-img: img/post-bg-os-metro.jpg
catalog: true
tags:
    - 大数据
    - hadoop
---

> hadoop的一个基本集群demo环境搭建.......

### 虚拟机集群网络(centos7)

集群搭建时先明确集群怎么搞，自己用的vbox，搭三台虚拟机，这个其实都没什么，主要是网络，在大学的时候老师教的做法是在虚拟机跑桥接网络，这样把虚拟机搞到一个局域网里面通讯而且能够跟外网通讯。

但是到了公司工作了，就会发现，这样的本地网络环境搭建是很困难的，网络都是一大堆代理+认证，根本分配不了ip，没法跟外网通信。

从走外网的角度出发，虚拟机可以走nat，但是走nat有个问题，虚拟机能走外网了，但是没办法走集群通讯。考虑局域集群的话，除了桥接，其实还可以走host-only网络，但是host-only走网络的虚拟机不可以跟外网通讯。

所以，在实作的时候可以考虑nat+host-only,一张网卡走nat，支持外网通讯，另一张走host-only网络，支持局域网通讯。

网卡1走nat

![](http://cyber-space.oss-cn-shenzhen.aliyuncs.com/git/vboxnet.png)

网卡2走host-only

![](http://cyber-space.oss-cn-shenzhen.aliyuncs.com/git/vbox2.png)

进入虚拟机查看

![](http://cyber-space.oss-cn-shenzhen.aliyuncs.com/git/net.png)

然后去将host-only的ip改为static

```
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no

BOOTPROTO=static
#BROADCAST=0.0.0.0
IPADDR=192.168.56.103
NETMASK=255.255.255.0
GETWAY=192.168.56.1

DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=enp0s3
UUID=f442f276-d55a-40fe-ba8e-cacc295a7a1b
DEVICE=enp0s8
ONBOOT=yes
```

配置三台这样的虚拟机

![](http://cyber-space.oss-cn-shenzhen.aliyuncs.com/git/%E7%BE%A4%E7%BB%84.png)







### 虚拟机配置

######　修改三台机子的host文件

```
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.56.103 ruby-master
192.168.56.104 ruby-slaver01
192.168.56.105 ruby-slaver02
```

###### 创建管理用户

```
在所有的主机下均建立一个账号admin用来运行hadoop ，并将其添加至sudoers中
[root@ruby-master  ~]# useradd admin    添加用户通过手动输入修改密码
[root@ruby-master  ~]# passwd  admin  更改用户 admin 的密码
 passwd： 所有的身份验证令牌已经成功更新。
设置admin用户具有root权限  修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示：
[root@ruby-master  ~]# visudo
 ####Allow root to run any commands anywhere
root    ALL=(ALL)     ALL
admin   ALL=(ALL)     ALL
修改完毕 :wq!保存退出，现在可以用admin帐号登录，然后用命令 su - ，切换用户即可获得root权限进行操作。
```

###### /opt**目录下创建文件夹并授权**

1. 在root用户下创建module、software文件夹

   ```
   [root@ruby-master opt]# mkdir module
   [root@ruby-master opt]# mkdir software
   ```

2. 修改module、software文件夹的所有者

   ```
   [root@ruby-master opt]# chown admin:admin module
   [root@ruby-master opt]# chown admin:admin software
   ```

3. 查看module、software文件夹的所有者

   ```
   [root@ruby-master opt]# ll
    total 
    drwxr-xr-x. 4 admin admin 180 Dec  9 20:10 module
    drwxr-xr-x. 2 admin admin   6 Nov 30 03:42 software
   ```

###### 安装Jdk

 **下载java1.8，解压，在/etc/profile添加配置**

```
export JAVA_HOME=/usr/local/jdk1.8.0_181`
export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar
export PATH=$JAVA_HOME/bin:$PATH 
```



### 安装hadoop集群

###### 节点规划

|   节点名称    |    NN    |     JJN     |    DN    | ZKFC | ZK        |       RM        |     NM      |
| :-----------: | :------: | :---------: | :------: | ---- | --------- | :-------------: | :---------: |
|  ruby-master  | NameNode | JournalNode | DataNode | ZKFC | Zookeeper |                 | NodeManager |
| ruby-slaver01 | NameNode | JournalNode | DataNode | ZKFC | Zookeeper | ResourceManager | NodeManager |
| ruby-slaver02 |          | JournalNode | DataNode |      | Zookeeper | ResourceManager | NodeManager |

###### ZK集群

**节点规划**

| 节点名称      | Zookeeper       | Jdk    |
| ------------- | --------------- | ------ |
| ruby-master   | Zookeeper3.4.12 | Jdk1.8 |
| ruby-slaver01 | Zookeeper3.4.12 | Jdk1.8 |
| ruby-slaver02 | Zookeeper3.4.12 | Jdk1.8 |

**安装**

解压zookeeper安装包到/opt/module/目录下

```
[admin@ruby-master module]$ tar -zxvf zookeeper-3.4.12.tar.gz -C /opt/module/
```

在/opt/module/zookeeper-3.4.12/这个目录下创建Data

```
[admin@ruby-master zookeeper-3.4.12]# sudo mkdir  Data
```

重命名/opt/module/zookeeper-3.4.12/conf这个目录下的zoo_sample.cfg为zoo.cfg

```
[admin@ruby-maste conf]# mv zoo_sample.cfg zoo.cfg 配置zoo.cfg文件
```

配**置zoo.cfg文件**

```
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just
# example sakes.
#dataDir=/tmp/zookeeper
dataDir=/opt/module/zookeeper-3.4.12/Data
dataLogDir=/opt/module/zookeeper-3.4.12/logs

# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1


server.1=ruby-master:2888:3888
server.2=ruby-slaver01:2888:3888
server.3=ruby-slaver02:2888:3888
```

**在/opt/module/zookeeper-3.4.12/Data目录下创建一个myid的文件**

编辑myid文件， 在文件中添加与server对应的编号：如 1 

```
[admin@ruby-master Data]# vi myid
```

**拷贝配置好的zookeeper到其他机器上**

```
[admin@ruby-master module]# scp -r zookeeper-3.4.12/ admin@ruby-slaver01:/opt/module/
[admin@ruby-master module]# scp -r zookeeper-3.4.12/ admin@ruby-slaver02:/opt/module/
```

**修改ruby-slaver01，ruby-slaver02中myid文件中内容为2、3**

```
[admin@ruby-slaver01 Data]# echo 2 > myid
[admin@ruby-slaver02 Data]# echo 3 > myid
```

**启动**

```
[admin@ruby-master zookeeper-3.4.12]# bin/zkServer.sh start
[admin@ruby-slaver01 zookeeper-3.4.12]# bin/zkServer.sh start
[admin@ruby-slaver02 zookeeper-3.4.12]# bin/zkServer.sh start
```

**查看**

```
[admin@ruby-slaver02 bin]$ zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper-3.4.12/bin/../conf/zoo.cfg
Mode: leader
======================================================================
[admin@ruby-slaver01 bin]$ zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper-3.4.12/bin/../conf/zoo.cfg
Mode: follower
======================================================================
[root@ruby-master bin]# zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper-3.4.12/bin/../conf/zoo.cfg
Mode: follower

```





###### SSH免密码配置

**关于ssh免密码的设置，要求每两台主机之间设置免密码，自己的主机与自己的主机之间也要求设置免密码。 这项操作可以在admin用户下执行，执行后公钥在/home/admin/.ssh/id_rsa.pub**

`[admin@ruby-master ~]# ssh-keygen -t rsa`
`[admin@ruby-master ~]# ssh-copy-id ruby-master`
`[admin@ruby-master ~]# ssh-copy-id ruby-slaver01`
`[admin@ruby-master ~]# ssh-copy-id ruby-slaver02`

**ruby-master与ruby-slaver01为namenode节点要相互免秘钥   HDFS的HA**

`[admin@ruby-slaver01 ~]# ssh-keygen -t rsa`
`[admin@ruby-slaver01 ~]# ssh-copy-id ruby-slaver01`
`[admin@ruby-slaver01 ~]# ssh-copy-id ruby-master`
`[admin@ruby-slaver01 ~]# ssh-copy-id ruby-slaver02`

**ruby-slaver01与ruby-slaver02为yarn节点要相互免秘钥  YARN的HA**

`[admin@ruby-slaver02 ~]# ssh-keygen -t rsa`
`[admin@ruby-slaver02 ~]# ssh-copy-id ruby-slaver02`
`[admin@ruby-slaver02 ~]# ssh-copy-id ruby-master`
`[admin@ruby-slaver02 ~]# ssh-copy-id ruby-slaver01`

###### 解压hadoop

`下载hadoop2.7.7解压/opt/module下面`

插一句，如果觉得文件共享麻烦的话，有个简单的方法，可以在vbox里面配置一个宿主的共享文件夹，这样的话所有的虚拟机都能共享读取文件了。

###### 修改**core-site.xml**

打开 /opt/module/hadoop-2.7.7/etc/hadoop目录

```
<configuration>
<!-- 把两个NameNode的地址组装成一个集群mycluster -->
<property>
   <name>fs.defaultFS</name>
   <value>hdfs://mycluster</value>
</property>
<!-- 指定hadoop运行时产生文件的存储目录 -->
<property>
  <name>hadoop.tmp.dir</name>
  <value>/opt/module/hadoop-2.7.7/data/ha/tmp</value>
</property>
<!-- 指定ZKFC故障自动切换转移 -->
<property>
     <name>ha.zookeeper.quorum</name>
     <value>ruby-master:2181,ruby-slaver01:2181,ruby-slaver02:2181</value>
</property>
</configuration>
```

###### 修改hadoop-env.sh，配置java环境

```
# The only required environment variable is JAVA_HOME.  All others are
# optional.  When running a distributed configuration it is best to
# set JAVA_HOME in this file, so that it is correctly defined on
# remote nodes.

# The java implementation to use.
export JAVA_HOME=/usr/local/jdk1.8.0_181
```

###### 修改hdfs-site.xml

```
<configuration>
<!-- 设置dfs副本数，默认3个 -->
<property>
<name>dfs.replication</name>
<value>2</value>
</property>
<!-- 完全分布式集群名称 -->
<property>
  <name>dfs.nameservices</name>
  <value>mycluster</value>
</property>
<!-- 集群中NameNode节点都有哪些 -->
<property>
   <name>dfs.ha.namenodes.mycluster</name>
   <value>nn1,nn2</value>
</property>
<!-- nn1的RPC通信地址 -->
<property>
   <name>dfs.namenode.rpc-address.mycluster.nn1</name>
   <value>ruby-master:8020</value>
</property>
<!-- nn2的RPC通信地址 -->
<property>
   <name>dfs.namenode.rpc-address.mycluster.nn2</name>
   <value>ruby-slaver01:8020</value>
</property>
<!-- nn1的http通信地址 -->
<property>
   <name>dfs.namenode.http-address.mycluster.nn1</name>
   <value>ruby-master:50070</value>
</property>
<!-- nn2的http通信地址 -->
<property>
    <name>dfs.namenode.http-address.mycluster.nn2</name>
    <value>ruby-slaver01:50070</value>
</property>
<!-- 指定NameNode元数据在JournalNode上的存放位置 -->
<property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://ruby-master:8485;ruby-slaver01:8485;ruby-slaver02:8485/mycluster</value>
</property>
<!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->
<property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
</property>
<!-- 使用隔离机制时需要ssh无秘钥登录-->
<property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/home/admin/.ssh/id_rsa</value>
</property>
<!-- 声明journalnode服务器存储目录-->
<property>
   <name>dfs.journalnode.edits.dir</name>
   <value>/opt/module/hadoop-2.7.7/data/ha/jn</value>
</property>
<!-- 关闭权限检查-->
<property>
   <name>dfs.permissions.enable</name>
   <value>false</value>
</property>
<!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式-->
<property>
   <name>dfs.client.failover.proxy.provider.mycluster</name>
   <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
<!-- 配置自动故障转移-->
<property>
   <name>dfs.ha.automatic-failover.enabled</name>
   <value>true</value>
</property>
</configuration>

```

###### **修改mapred-site.xml**

```
<configuration>
<!-- 指定mr框架为yarn方式 -->
　<property>
　　<name>mapreduce.framework.name</name>
　　<value>yarn</value>
　</property>
<!-- 指定mr历史服务器主机,端口 -->
  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>ruby-master:10020</value>
  </property>
<!-- 指定mr历史服务器WebUI主机,端口 -->
  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>ruby-master:19888</value>
  </property>
<!-- 历史服务器的WEB UI上最多显示20000个历史的作业记录信息 -->
  <property>
    <name>mapreduce.jobhistory.joblist.cache.size</name>
    <value>20000</value>
  </property>
<!--配置作业运行日志 -->
  <property>
    <name>mapreduce.jobhistory.done-dir</name>
    <value>${yarn.app.mapreduce.am.staging-dir}/history/done</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.intermediate-done-dir</name>
    <value>${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate</value>
  </property>
  <property>
    <name>yarn.app.mapreduce.am.staging-dir</name>
    <value>/tmp/hadoop-yarn/staging</value>
  </property>
</configuration>
```

######  **修改** **slaves**

```
ruby-master
ruby-slaver01
ruby-slaver02
```

###### **修改yarn-site.xml**

><configuration>
><!-- reducer获取数据的方式 -->
> <property>
>        <name>yarn.nodemanager.aux-services</name>
>        <value>mapreduce_shuffle</value>
>    </property>
>    <!--启用resourcemanager ha-->
>    <property>
>        <name>yarn.resourcemanager.ha.enabled</name>
>        <value>true</value>
>    </property>
>    <!--声明两台resourcemanager的地址-->
>    <property>
>        <name>yarn.resourcemanager.cluster-id</name>
>        <value>rmCluster</value>
>    </property>
>    <property>
>        <name>yarn.resourcemanager.ha.rm-ids</name>
>        <value>rm1,rm2</value>
>    </property>
>    <property>
>        <name>yarn.resourcemanager.hostname.rm1</name>
>        <value>ruby-slaver01</value>
>    </property>
>    <property>
>        <name>yarn.resourcemanager.hostname.rm2</name>
>        <value>ruby-slaver02</value>
>    </property>
>    <!--指定zookeeper集群的地址-->
>    <property>
>        <name>yarn.resourcemanager.zk-address</name>
>        <value>ruby-master:2181,ruby-slaver01:2181,ruby-slaver02:2181</value>
>    </property>
>    <!--启用自动恢复-->
>    <property>
>        <name>yarn.resourcemanager.recovery.enabled</name>
>        <value>true</value>
>    </property>
>    <!--指定resourcemanager的状态信息存储在zookeeper集群-->
>    <property>
>        <name>yarn.resourcemanager.store.class</name>
>        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
>    </property>
></configuration>



 ###### **拷贝hadoop到其他节点**

```
[admin@ruby-master module]# scp -r hadoop-2.7.6/ admin@ruby-slaver01:/opt/module/
[admin@ruby-master module]# scp -r hadoop-2.7.6/ admin@ruby-slaver02:/opt/module/
```

###### **Hadoop环境变量**

```
[admin@ruby-master ~]$ sudo vi /etc/profile
末尾追加
export  HADOOP_HOME=/opt/module/hadoop-2.7.6
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
编译生效  source  /etc/profile
```

###### 启动

**在各个JournalNode节点上，输入以下命令启动journalnode服务：（前提zookeeper集群已启动）**

```
[admin@ruby-master ~]$ hadoop-daemon.sh start journalnode
[admin@ruby-slaver01 ~]$ hadoop-daemon.sh start journalnode
[admin@ruby-slaver02 ~]$ hadoop-daemon.sh start journalnode
```

**在[ruby-master]上，对namenode进行格式化，并启动：**

```
[admin@ruby-master ~]$ hdfs namenode -format
```

**启动ruby-master上namenode**

```
[admin@ruby-master sbin]$ hadoop-daemon.sh  start namenode
```

**在ruby-slaver01上，同步ruby-master的元数据信息：**

```
[admin@ruby-slaver01 ~]$ hdfs namenode -bootstrapStandby
```

**启动ruby-slaver01：**

```
admin@ruby-slaver01 sbin]$ hadoop-daemon.sh start namenode
```

**在ruby-master上，启动所有datanode**

```
[admin@ruby-master sbin]$ hadoop-daemons.sh start datanode
```

**在各个NameNode节点上启动DFSZK Failover Controller，先在哪台机器启动，哪个机器的NameNode就是Active NameNode**

```
[admin@ruby-slaver01 sbin]$ hadoop-daemin.sh start zkfc
[admin@ruby-master sbin]$ hadoop-daemin.sh start zkfc
```

 **在宿主机上访问**

![](http://cyber-space.oss-cn-shenzhen.aliyuncs.com/git/master.png)



![](http://cyber-space.oss-cn-shenzhen.aliyuncs.com/git/masterAc.png)

**启动yarn**

在ruby-slaver01中执行：

```
[admin@ruby-slaver01 sbin]$ start-yarn.sh
```

在ruby-slaver02中执行：

```
[admin@ruby-slaver02 ~]$ yarn-daemon.sh start resourcemanager　
```

 **查看状态**

```
[admin@ruby-slaver01 bin]$ yarn rmadmin -getServiceState rm1
active
[admin@ruby-slaver01 bin]$ yarn rmadmin -getServiceState rm2
standby
```

**宿主机查看**

![](http://cyber-space.oss-cn-shenzhen.aliyuncs.com/git/hadoop.png)

**查看进程**

```
[admin@ruby-master module]$ jps
8368 Jps
4225 DFSZKFailoverController
4421 NodeManager
4535 JobHistoryServer
3433 NameNode
3930 JournalNode
3612 DataNode
2543 QuorumPeerMai
=============================================
[admin@ruby-slaver01 bin]$ jps
3424 NodeManager
1876 QuorumPeerMain
2821 JournalNode
2615 DataNode
3079 DFSZKFailoverController
3272 ResourceManager
8584 Jps
2458 NameNode
============================================
[admin@ruby-slaver02 sbin]$ jps
3810 DataNode
3908 JournalNode
3015 QuorumPeerMain
1463 Jps
4011 NodeManager
19327 ResourceManager
```





















































































































